#summary Notes on feedback-directed optimization

*These are currently just notes, and reflect plans for Q3. This will be edited to be clearer, and updated as features are implemented.*

We wish to make use of the wealth of information available at runtime to optimize Python programs. Sophisticated implementations of Self, Java and JavaScript have demonstrated the real-world applicability of these techniques, and Psyco has demonstrated some applicability to Python. Accordingly, we believe it will be profitable to make use of runtime information when optimizing Unladen Swallow's native code generation.

== Background ==

Unladen Swallow compiles Python code to a bytecode representation that is amenable to reasonably-performant execution in a custom virtual machine. Once a piece of code has been deemed hot, we compile the bytecode to native code using LLVM. It is this native code that we seek to optimize. Optimizing the execution of the generated bytecode is less interesting since the system should be selecting most performance-critical functions for compilation to native code. However, modifications to the bytecode to enable easier compilation/profiling are fair game.

== Points ==
  * We wish to gather as much information at runtime as possible, not merely type information (as implied by the more specific name "type feedback"). The representation used should allow for sufficient flexibility to record function pointer addresses, branch-taken statistics, etc, potentially any and every piece of information available at runtime.
  * The gathered information should live in the code object so that it lasts as long as the relevant bytecode.
  * Hölzle 1994 includes an analysis showing that the vast majority of Self call sites are monomorphic (see section 3). Recent [http://gist.github.com/134687 analysis of Ruby programs] has observed a similar distribution of call site arity in Ruby. We believe that Python programs are sufficiently similar to Self and Ruby in this regard. Based on these findings from other languages, we will want to limit our optimization attempts to call sites with arity < 3. Our implementation of feedback-directed optimization should gather enough data to conduct a similar analysis for Python.
  * Due to the nature of Python's bytecode format, we believe it would be unprofitable to implement the desired level of data gathering inline, that is, as separate opcodes. Instead, the bodies of interesting opcodes in the Python VM will be modified to record the data they use. This will be faster (avoids opcode fetch/dispatch overhead) and easier to reason about (no need to track multi-byte opcode sequences).
  * We will optimize for the common case, as determined by the data-gathering probes we will add to interesting opcodes. Guards will detect the uncommon case and fail back to the interpreter. This allows our assumptions about the common case to propagate to later code. Again, see Hölzle 1994.

== Places we would like to optimize (non-exhaustive) ==
  * Math. If both operands are ints, we would like to inline the math operations into the generated machine code, rather than going through indirect calls. If both operands are strings, we would like to call directly to the appropriate function (possibly compiled with Clang and using the fastcc calling convention) rather than going through the indirection in `PyNumber_Add()`.
  * UNPACK_SEQUENCE. Knowing the type of the sequence being unpacked could allow us to inline the subsequent STORE_FOO opcodes and avoid a lot of stack manipulation.
  * Calls to builtins. We would like to be able to inline calls to builtin functions, or at the very least, avoid looking the function up via LOAD_GLOBAL. Ideally we would also be able to inline some of these calls where that is deemed profitable. For example, inlining `len()` could save not only the LOAD_GLOBAL lookup but also the layers of indirection incurred in `PyObject_Size()`. In the best case, a call to `len()` on lists or tuples (or other builtin types) could be turned into `((PyVarObject *)(ob))->ob_size`.
  * Branches. If a branch is always taken in a given direction, we can omit the machine code for the uncommon case, falling back to the interpreter instead. This can be used to simplify the control-flow graph and thus allow greater optimization of the common case and the code that follows it.
  * Method dispatch. If we know the most-likely receiver types for a given method invocation, we can potentially avoid the method lookup overhead or inline the call entirely. Note that in Python 2.6 and higher, method lookups are cached in the type objects so the potential savings of skipping some steps in the cache check process may be minimal. Better to reuse this information for possible inlining efforts.
  * Function calls. If we know the parameter signature of the function being invoked, we can avoid the overhead of taking the arguments and matching them up with the formal parameters. This logic can be fairly expensive, since it is designed to be as general as possible to support the wide variety of legal function call/definition combinations. If we don't need to be so general, we can be faster.