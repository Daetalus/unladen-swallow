#summary Description of the benchmarks used for Unladen Swallow.

= Unladen Swallow Benchmarks =

The Unladen Swallow benchmark suite is kept in the [http://code.google.com/p/unladen-swallow/source/browse/#svn/tests tests/] directory. `tests/perf.py` is the main interface to the tests, with the individual benchmarks stored under the `tests/performance/` directory.


Example `perf.py` command:

{{{
python2.5 tests/perf.py --rigorous --benchmarks=2to3,django control/python experiment/python
}}}

This will run the 2to3 and Django benchmarks in rigorous mode (lots of iterations), taking `control/python` as the baseline and `experiment/python` as the binary you've been mucking around with. `perf.py` will take care of comparing the performance and running statistics on the result to determine statistical significance. The performance degradation/improvement is calculated using `((new - old) / new)`.


==Benchmarks==

  * *2to3* - have [http://svn.python.org/view/sandbox/trunk/2to3/ the 2to3 tool] translate itself.
  * *Django* - use the [http://www.djangoproject.com/ Django template system] to build a 150x150-cell HTML table.
  * *Pickle* - use the `cPickle` module to pickle a variety of datasets.
  * *PyBench* - run the standard Python [http://svn.python.org/projects/python/trunk/Tools/pybench/README PyBench] benchmark suite. This is considered an unreliable, unrepresentative benchmark; do not base decisions off it. It is included only for completeness.
  * *SlowPickle* - use the pure-Python `pickle` module to pickle a variety of datasets.
  * *SlowSpitfire* - use the [http://code.google.com/p/spitfire/ Spitfire template system] to build a 1000x1000-cell HTML table. Unlike the *Spitfire* benchmark listed below, *SlowSpitfire* does not use Pysco.
  * *SlowUnpickle* - use the pure-Python `pickle` module to unpickle a variety of datasets.
  * *Spitfire* - use the [http://code.google.com/p/spitfire/ Spitfire template system] to build a 1000x1000-cell HTML table, taking advantage of Psyco for acceleration.
  * *Unpickle* - use the `cPickle` module to unpickle a variety of datasets.

==Benchmarks we don't use==

We do not include [http://svn.python.org/view/python/trunk/Tools/pybench/ PyBench], [http://code.google.com/p/unladen-swallow/source/browse/tests/performance/pystone.py PyStone] or [http://code.google.com/p/unladen-swallow/source/browse/tests/performance/richards.py Richards] in our active benchmark suite. PyStone and Richards are synthetic benchmarks that may or may not translate into improved performance for real-world applications. We would like to avoid basing decisions on PyStone or Richards, only to find out that a real application sees no benefit -- or worse, is slowed down. In both cases, these benchmarks have a long history and have gone through many translations: PyStone was originally written in Ada, then translated to C, then translated to Python and does not represent idiomatic Python code or its performance hot spots. Richards was originally written in BCPL, then translated to Smalltalk, then to C++, then to Java and finally to Python; it does a little better at testing OO performance, but doesn't involve string processing at all, something that many Python applications rely on heavily. Also, it is not idiomatic Python code.

While PyBench may be an acceptable collection of microbenchmarks, it is not a reliable or precise benchmark. We have observed swings of up to 10% between runs on unloaded machines using the same version of Python; we would like to detect performance differences of 1% accurately. For us, the final nail in PyBench's coffin was when experimenting with gcc's feedback-directed optimization tools, we were able to produce a universal 15% performance increase across our macrobenchmarks; using the same training workload, PyBench got 10% slower. For this reason, we do not factor in PyBench results to our decision-making.