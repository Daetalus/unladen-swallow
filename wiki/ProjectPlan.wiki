#summary Plans for optimizing Python
#labels Featured

<font size="1">NB: all cited papers are linked on [RelevantPapers]. A version of this document is also [http://danmarner.yo2.cn/unladen-swallow-project-plan/ available in Chinese], though we can't vouch for the translation.</font>

<wiki:toc max_depth="3" />

= Goals =

We want to make Python faster, but we also want to make it easy for large, well-established applications to switch to Unladen Swallow.

  # Produce a version of Python at least 5x faster than CPython.
  # Python application performance should be stable.
  # Maintain source-level compatibility with CPython applications.
  # Maintain source-level compatibility with CPython extension modules.
  # We do not want to maintain a Python implementation forever; we view our work as a branch, not a fork.


= Overview =

In order to achieve our combination of performance and compatibility goals, we opt to modify CPython, rather than start our own implementation from scratch. In particular, we opt to start working on CPython 2.6.1: Python 2.6 nestles nicely between 2.4/2.5 (which most interesting applications are using) and 3.x (which is the eventual future). Starting from a CPython release allows us to avoid reimplementing a wealth of built-in functions, objects and standard library modules, and allows us to reuse the existing, well-used CPython C extension API. Starting from a 2.x CPython release allows us to more easily migrate existing applications; if we were to start with 3.x, and ask large application maintainers to first port their application, we feel this would be a non-starter for our intended audience.

The majority of our work will focus on speeding the execution of Python code, while spending comparatively little time on the Python runtime library. Our long-term proposal is to supplement CPython's custom virtual machine with a JIT built on top of [http://llvm.org/ LLVM], while leaving the rest of the Python runtime relatively intact. We have observed that Python applications spend a large portion of their time in the main eval loop. In particular, even relatively minor changes to VM components such as opcode dispatch have a significant effect on Python application performance. We believe that compiling Python to machine code via LLVM's JIT engine will deliver large performance benefits.

Some of the obvious benefits:
<ul>
  <li>Using a JIT will also allow us to move Python from a stack-based machine to a register machine, which has been shown to improve performance in other similar languages (Ierusalimschy et al, 2005; Shi et al, 2005).</li>
  <li>Eliminating the need to fetch and dispatch opcodes should alone be a win, even if we do nothing else. See http://bugs.python.org/issue4753 for a discussion of CPython's current sensitivity to opcode dispatch changes.</li>
  <li>The current CPython VM opcode fetch/dispatch overhead makes implementing additional optimizations prohibitive. For example, we would like to implement type feedback and dynamic recompilation ala SELF-93 (HÃ¶lzle, Chambers and Ungar, 1992), but we feel that implementing the polymorphic inline caches in terms of CPython bytecode would be unacceptably slow.</li>
  <li>LLVM in particular is interesting because of its easy-to-use codegen available for multiple platforms and its ability to compile C and C++ to the same intermediate representation we'll be targeting with Python. This will allows us to do inlining and analysis across what is currently a Python/C language barrier.</li>
</ul>

With the infrastructure to generate machine code comes the possibility of compiling Python into a much more efficient implementation than what would be possible in the current bytecode-based representation. For example, take the snippet

{{{
for i in range(3):
  foo(i)
}}}
This currently desugars to something like
{{{
$x = range(3)
while True:
  try:
    $y = $x.next()
  except StopIteration:
    break
  i = $y
  foo(i)
}}}
Once we have a mechanism to know that `range()` means the `range()` builtin function, we can turn this into something more akin to
{{{
for (i = 0; i < 3; i++)
  foo(i)
}}}
in C, possibly using unboxed types for the math. We can then unroll the loop to yield
{{{
foo(0)
foo(1)
foo(2)
}}}

We intend to structure Unladen Swallow's internals to assume that multiple cores are available for our use. Servers are only going to acquire more and more cores, and we want to exploit that to do more and more work in parallel. For example, we would like to have a concurrent code optimizer that applies increasingly-expensive (and -beneficial!) optimizations in parallel with code execution, using another core to do the work. We are also considering [GarbageCollector a concurrent garbage collector] that would, again, utilize additional cores to offload work units. Since most production server machines are shipping with between 4 and 32 cores, we believe this avenue of optimization is potentially lucrative. However, we will have to be sensitive to the needs of highly-parallel applications and not consume extra cores blindly.

Note that many of the areas we will need to address have been considered and developed by the other dynamic language implementations like [http://www.macruby.org/blog/2009/03/28/experimental-branch.html MacRuby], [http://jruby.codehaus.org/ JRuby], [http://rubini.us/ Rubinius] and [http://www.parrot.org/ Parrot], and in particular other Python implementations like [http://www.jython.org/Project/ Jython], [http://codespeak.net/pypy/dist/pypy/doc/ PyPy], and [http://www.codeplex.com/Wiki/View.aspx?ProjectName=IronPython IronPython]. In particular, we're looking at these other implementations for ideas on debug information, regex performance ideas, and generally useful performance ideas for dynamic languages. This is all fairly well-trodden ground, and we want to avoid reinventing the wheel as much as possible.


= Milestones =

Unladen Swallow will be released every three months, with bugfix releases in between as necessary.

== 2009 Q1 ==

Q1 will be spent making relatively minor tweaks to the existing CPython implementation. We aim for a 25-35% performance improvement over our baseline. Our goals for this quarter are conservative, and are aimed at delivering tangible performance benefits to client applications as soon as possible, that is, without waiting until the completion of the project.

Ideas for achieving this goal:
  * Re-implement the eval loop in terms of [http://www.complang.tuwien.ac.at/anton/vmgen/ vmgen].
  * Experiment with compiler options such as 64 bits, LLVM's LTO support, and gcc 4.4's FDO support.
  * Replace rarely-used opcodes with functions, saving critical code space.
  * Improve GC performance (see http://bugs.python.org/issue4074).
  * Improve cPickle performance. Many large websites use this heavily for interacting with memcache.
  * Simplify frame objects to make frame alloc/dealloc faster.
  * Implement one of the several proposed schemes for speeding lookups of globals and builtins.

The 2009Q1 release can be found in the [http://code.google.com/p/unladen-swallow/source/browse/branches/release-2009Q1-maint release-2009Q1-maint branch]. See [Releases] for our performance relative to CPython 2.6.1.

== 2009 Q2 ==

Q2 will focus on supplementing the Python VM with a functionally-equivalent implementation in terms of LLVM. We anticipate some performance improvement, but that is not the primary focus of the 2009Q2 release. We will focus on just getting something working on top of LLVM. Making it faster will come in subsequent quarters.

Goals:
  * Addition of an LLVM-based JIT.
  * All tests in the standard library regression suite pass when run via the JIT.
  * Source compatibility maintained with existing applications, C extension modules.
  * 10% performance improvement.
  * Stretch goal: 25% performance improvement.


== 2009 Q3 and Beyond ==

The plan for Q3 onwards is to simply iterate over [RelevantPapers the literature]. We aspire to do no original work, instead using as much of the last 30 years of research as possible. See RelevantPapers for a partial (by no means complete) list of the papers we plan to implement or draw upon.

We plan to address performance considerations in the regular expression engine, as well as any other extension modules found to be bottlenecks. However, regular expressions are already known to be a good target for our work and will be considered first for optimization.

In addition, we intend to remove the GIL and fix the state of multithreading in Python. We believe this is possible through the implementation of a more sophisticated GC system, something like IBM's Recycler (Bacon et al, 2001).

Our long-term goal is to make Python fast enough to start moving performance-important types and functions from C back to Python.

The exact performance targets for the 2009Q3 release will be finalized some time in Q2.


= Detailed Plans =

Once we get the quick improvements out of the way and convince some people to adopt our Python, we'll start switching to LLVM, probably initially in a separate branch. This can go in lots of stages:


<ol>
<li>(Finished) Add `llvmmodule` and `llvmfunction` types in Python, which interface to the LLVM [http://llvm.org/docs/ProgrammersManual.html#Module Module] and [http://llvm.org/docs/ProgrammersManual.html#Function Function] types. Teach `llvmmodule` to [http://llvm.org/doxygen/ReaderWriter_8h.html read bitcode files] and [http://llvm.org/doxygen/PrintModulePass_8h.html pretty-print them]. (Reading the bitcode files isn't really necessary for the next step, but it'll help with testing this one.)

[http://code.google.com/p/llvm-py/ llvm-py] has such wrapper types, but using Python-defined types in the core interpreter is very difficult, so we'll write our own. These won't be full wrappers (leave that to llvm-py). Just what's convenient and useful for debugging.</li>

<li>(Finished) Add a way to run Python functions that are backed by LLVM IR. We used the LLVM JIT to compile and run these functions. This got things working really quickly, but there seem to be some [CodeLifecycle lifetime issues] which we need to work out.</li>

<li>(In progress) Make the `compile` builtin generate LLVM IR (as the `llvmmodule` or `llvmfunction` type) in addition to CPython bytecode. We can get this working incrementally and keep a working Python interpreter the whole time. Our progress is visible in the list of UNIMPLEMENTED opcodes in [http://code.google.com/p/unladen-swallow/source/browse/trunk/Python/ll_compile.h Python/ll_compile.h]. We've been translating each opcode into LLVM IR mostly manually, although where we can factor functions out and just call them, we do that.
Several Python constructs may give us trouble here:
<ul>
<li>Closures: A closure is just a function with a pre-bound parameter representing the environment.</li>
<li>ExceptionHandling -- literally transcribing the ceval exception handling mechanism is working fine so far. CPython saves bytecode indices as jump targets sometimes. Since LLVM doesn't let us save instruction addresses, we instead number the basic blocks that are used as exception handlers and emit a switch statement to jump to the appropriate one.</li>
<li>The FunctionCallingConvention -- We pass a PyFrameObject containing all the arguments. Besides being more expensive to create than we'd like, the PyFrameObject makes things like the stack and local variables escape when the function returns, which prevents LLVM from optimizing as much as it should.</li>
<li>Generators: We intend to number yield statements much like we numbered exception handlers and emit a switch statement in the function's entry block to resume where a yield left off. We need to save all registers and local variables across a yield call. Fortunately, with the current literal translation from CPython bytecode, the stack and local variables are all saved into the FrameObject at a return, which should make it really easy to resume the yield.  This will get much harder when/if we stop saving locals into the frame all the time. We'd have to save and restore all the live pseudo-registers at each yield, but LLVM doesn't currently have an intrinsic to do that.</li>
</ul>
</li>

<li>Probably separate the LLVM IR generation back out of the CPython compiler. Generating IR for every function appears to be quite expensive in terms of memory and time. If we compile only hot functions (like every other JITted system does) we'll probably get better performance. We'll need to measure both options before doing this.</li>

<li>Figure out the simplest possible version of [PersistentIr .pyc persistence]. I'm hoping the obvious thing -- dump the python module as an LLVM Module in bitcode to disk -- just works. We can add the fancy profiling data and pre-optimized code later (step 11).</li>

<li>Teach Python how to decide whether to fast-JIT or slow-JIT a particular function (pass fast==true to [http://llvm.org/doxygen/classllvm_1_1JIT.html#fd0a3b0d55489b96760121e3e25b7de6 JIT::create]). nlewycky says, "The codegen is roughly 3x faster in fast mode (note: measured in debug mode not release mode, so who knows).". I imagine that we'll have to tune a bit to find the right balance. At this point I expect Python to get faster than our original release.</li>

<li>Start adding [LlvmOptimizations optimizations].

Talin points out that it'll be hard to apply most of the pre-defined optimizations at first because Python calls are indirect. This motivates type inference so we can make the calls direct.</li>
</ol>

8+ can be tried in parallel, probably starting toward the end of 2009Q2.

<ol start="8">
<li>Depending on the outcome of step 4, either consider re-doing the CPython bytecode to facilitate better profiling and compilation to llvm IR, or [DirectLlvmIrGeneration optimize the direct compilation to LLVM IR] using AST information.</li>

<li>Figure out how to specialize functions for particular types, which allows better inlining. We should be able to do as well as psyco pretty easily since we can steal their algorithms.
<ol type="a">
<li>Allow users to annotate functions to tell the interpreter which specializations to use.</li>
<li>Profile calls and [TypeInference infer common argument types].</li>
</ol></li>

<li>9b and 10 require us to be able to keep multiple versions of code around for any given function and backpatch existing callers. We'll probably want to [GarbageCollectingCode GC the multiple versions], which should be interesting.</li>

<li>[PersistentIr Persist the generated and optimized IR to disk] in place of .pyc
files, perhaps along with any gathered profile data. This allows subsequent runs to take advantage of the optimization work previous runs have already done. Matches LLVM's promise to be "A Compilation Framework for Lifelong Program Analysis & Transformation". :)</li>
</ol>


== Regular Expressions ==

While regexes aren't a traditional performance hotspot, we've found that most regex users expect them to be faster than they are, resulting in surprising performance degradation. For this reason, we'd like to invest some resources in speeding up CPython's regex engine.

CPython's current regex engine is a stack-based bytecode interpreter. It does not take advantage of any form of modern techniques to improve opcode dispatch performance (Bell 1973; Ertl & Gregg, 2003; Berndl et al, 2005) and is in other respects a traditional, straightforward virtual machine. We believe that many of the techniques being applied to speed up pure-Python performance are equally applicable to regex performance, starting at improved opcode dispatch all the way through JIT-compiling regexes down to machine code.

Recent work in the Javascript community has confirmed our belief. Google's V8 engine now includes [http://blog.chromium.org/2009/02/irregexp-google-chromes-new-regexp.html Irregexp], a JIT regex compiler, and the new [http://webkit.org/blog/214/introducing-squirrelfish-extreme/ SquirrelFish Extreme] includes a new regex engine based on the same principle: trade JIT compilation time for execution time. Both of these show impressive gains on the regex section of the various Javascript benchmarks. We would like to replicate these results for CPython.

We also considered using Thompson NFAs for very simple regexes, as [http://209.85.173.132/search?q=cache:XQrcPV-4kngJ:swtch.com/~rsc/regexp/regexp1.html+thompson+NFA&cd=1&hl=en&ct=clnk&gl=us advocated by Russ Cox]. This would create a multi-engine regex system that could choose the fastest way of implementation any given pattern. The V8 team also considered such a hybrid system when working on Irregexp but rejected it, [http://blog.chromium.org/2009/02/irregexp-google-chromes-new-regexp.html#c4843826268005492354 saying]

 The problem we ran into is that not only backreferences but also basic operators like `|` and `*` are defined in terms of backtracking. To get the right behavior you may need backtracking even for seemingly simple regexps. Based on the data we have for how regexps are used on the web and considering the optimizations we already had in place we decided that the subset of regexps that would benefit from this was too small.

One problem that needs to be overcome before any work on the CPython regex engine begins is that Python lacks a regex benchmark suite. We might be able to reuse [http://v8.googlecode.com/svn/data/benchmarks/v3/regexp.js the regexp.js component of the V8 benchmarks], but we would first need to verify that these are representative of the kind of regular expressions written in Python programs. We have no reason to believe that regexes used in Python programs differ significantly from those written in Javascript, Ruby, Perl, etc programs, but we would still need to be sure.


== Start-up Time ==

In talking to a number of heavy Python users, we've gotten a lot of interest in improving Python's start-up time. This comes from both very large-scale websites (who want faster server restart times) and from authors of command-line tools (where Python start time might dwarf the actual work done).

Start-up time is currently dominated by imports, especially for large applications like [http://bazaar-vcs.org/ Bazaar]. Python offers a lot of flexibility by deferring imports to runtime and providing a lot of hooks for configuring exactly how imports will work and where modules can be imported from. The price for that flexibility is slower imports.

For large users that don't take advantage of that flexibility -- in particular servers, where imports shouldn't change between restarts -- we might provide a way to opt in to stricter, faster import semantics. One idea is to ship all required modules in a single, self-contained "binary". This would both a) avoid multiple filesystem calls for each import, and b) open up the possibility of Python-level [http://www.airs.com/blog/archives/100 link-time optimization], resulting in faster code via inter-module inlining and similar optimizations. Self-contained images like this would be especially attractive for large Python users in the server application space, where hermetic builds and deployments are already considered essential.

A less invasive optimization would be speed up [http://docs.python.org/library/marshal.html Python's marshal module], which is used for `.pyc` and `.pyo` files. Based on Unladen Swallow's work speeding up `cPickle`, similarly low-hanging fruit probably exists in `marshal`.

We already have benchmarks tracking start-up time in a number of configurations. We will probably also add microbenchmarks focusing specifically on imports, since imports currently dominate CPython start time.


= Testing and Measurement =

== Performance ==

Unladen Swallow maintains a directory of interesting performance tests under the [http://code.google.com/p/unladen-swallow/source/browse/#svn/tests tests] directory. `perf.py` is the main interface to the benchmarks we care about, and will take care of priming runs, clearing `*.py[co]` files and running interesting statistics over the results.

Unladen Swallow's benchmark suite is focused on the hot spots in major Python applications, in particular web applications. The major web applications we have surveyed have indicated that they bottleneck primarily on template systems, and hence our initial benchmark suite focuses on them:
  * *Django and Spitfire templates*. Two very different ways of implementing a template language.
  * *2to3*. Translates Python 2 syntax to Python 3. Has an interesting, pure-Python kernel that makes heavy use of objects and method dispatch.
  * *Pickling and unpickling*. Large-scale web applications rely on memcache, which in turns uses Python's Pickle format for serialization.

There are also a number of microbenchmarks, for example, an N-Queens solver, an alphametics solver and several start-up time benchmarks.

Apart from these, our benchmark suite includes several crap benchmarks like Richards, PyStone and PyBench; these are only included for completeness and comparison with other Python implementations, which have tended to use them. Unladen Swallow does not consider these benchmarks to be representative of real Python applications or Python implementation performance, and does not run them by default or make decisions based on them.

For charting the long-term performance trend of the project, Unladen Swallow makes use of Google's standard internal performance measurement framework. Project members will post regular performance updates to the mailing lists. For testing individual changes, however, using `perf.py` as described on the [Benchmarks] page is sufficient.

== Correctness ==

In order to ensure correctness of the implementation, Unladen Swallow uses both the standard Python test suite, plus a number of third-party libraries that are known-good on Python 2.6. In particular, we test third-party C extension modules, since these are the easiest to break via unwitting changes at the C level.

As work on the JIT implementation moves forward, we will incorporate a fuzzer into our regular test run. We plan to reuse Victor Stinner's [http://fusil.hachoir.org/svn/trunk/ Fusil] Python fuzzer as much as possible, since it a) exists, and b) has been demonstrated to find real bugs in Python.

Unladen Swallow will come with a `--jit` option that can be used to control when the JIT kicks in. For example, `--jit=never` would disable the JIT entirely, while `--jit=always` would skip the warm-up interpreted executions and jump straight into native code generation; `--jit=once` will disable recompilation. These options will be used to test the various execution strategies in isolation. Our goal is to avoid JIT bugs that are never encountered because the buggy function isn't hot enough, as have been observed in the JVM (likewise for the interpreted mode).

Unladen Swallow maintains a [http://buildbot.net/trac BuildBot] instance that runs the above tests against every commit to trunk.

== Complexity ==

One of CPython's virtues is its simplicity: modifying CPython's VM and compiler is relatively simple and straight-forward. Our work with LLVM will inevitably introduce more complexity into CPython's implementation. In order to measure the productivity trade-offs that may result from this extra machinery, the Unladen Swallow team will periodically take ideas from the `python-dev` and `python-ideas` mailing lists and implement them. If implementation is significantly more difficult that the corresponding change to CPython, that's obviously something that we'll need to address before merger. We may also get non-team members to do the implementations so that we get a less biased perspective.


= Risks =

  * *May not be able to merge back into mainline.* There are vocal, conservative senior members of the Python core development community who may oppose the merger of our work, since it will represent such a significant change. This is a good thing! Resistance to change can be very healthy in situations like this, as it will force a thorough, public examination of our patches and their possible long-term impact on the maintenance of CPython -- this is open source, and another set of eyes is always welcome. We believe we can justify the changes we're proposing, and by keeping in close coordination with Guido and other senior members of the community we hope to limit our work to only changes that have a good chance of being accepted. However: there is still the chance that some patches will be rejected. Accordingly, we may be stuck supporting a de facto separate implementation of Python, or as a compromise, not being as fast as we'd like. C'est la vie.
  * *LLVM comes with a lot of unknowns:*  Impact on extension modules? JIT behaviour in multithreaded apps? Impact on Python start-up time?
  * *Windows support:* CPython currently has good Windows support, and we'll have to maintain that in order for our patches to be merged into mainline. Since none of the Unladen Swallow engineers have any/much Windows experience or even Windows machines, keeping Windows support at an acceptable level may slow down our forward progress or force us to disable some performance-beneficial code on Windows. Community contributions may be able to help with this.
  * *Specialized platforms:* CPython currently runs on a wide range of hardware and software platforms, from big iron server machines down to Nokia phones. We would like to maintain that kind of hard-won portability and flexibility as much as possible. We already know that LLVM (or even a hand-written JIT compiler) will increase memory usage and Python's binary footprint, possibly to a degree that makes it prohibitive to run Unladen Swallow on previously-support platforms. To mitigate this risk, Unladen Swallow will include a `./configure` flag to disable LLVM integration entirely and forcing the use of the traditional eval loop.


= Lessons Learned =

This section attempts to list the ways that our plans have changed as work has progressed, as we've read more papers and talked to more people. This list is incomplete and will only grow. 

  * Early in our planning, we had considered completely removing the custom CPython virtual machine and replacing it with LLVM. The benefit would be that there would be less code to maintain, and only a single encoding of Python's semantics. The theory was that we could either a) generate slow-to-run but fast-to-compile machine code, or b) generate LLVM IR and run it through LLVM's interpreter. Both of these turned out to be impractical: even at its fastest (using no optimization passes and LLVM's `FastISel` instruction selector), compiling to native code was too slow. LLVM's IR interpreter was both too slow and did not support all of LLVM's IR.<br><br>Preserving the CPython VM also allows Unladen to keep compatibility with the unfortunate number of Python packages that parse CPython bytecode.


= Communication =

All communication about Unladen Swallow should take place on the [http://groups.google.com/group/unladen-swallow Unladen Swallow list]. This is where design issues will be discussed, as well as notifications of continuous build results, performance numbers, code reviews, and all the other details of an open-source project. If you add a comment below, on this page, we'll probably miss it. Sorry. Please mail our list instead.