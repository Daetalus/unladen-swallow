#summary Plans for optimizing Python

= Quick Optimizations =

Q1 will likely be dominated by getting a 30%-50% improvement with
existing approaches. These will probably include compiling Python with
LLVM+LTO (10%), [VmgenEvalLoop vmgen] (10%(-30%??)), [SixtyFourBitCompile 64 bits] (guess -20-+20%) and
other small ceval, function call, etc. tweaks. I'll probably also
start pushing to [SimplifyEvalFrameEx simplify the EvalFrameEx loop] even if that doesn't
help performance because it'll make migrating to LLVM easier.

See ShortTermPerformanceIdeas for some of the ideas for this first phase.

= Long-term Plan =
Once we get the quick improvements out of the way and convince some
people to adopt our Python, we'll start switching to LLVM, probably
initially in a separate branch. This can go in lots of stages:

<ol>
<li>Add `llvmmodule` and `llvmfunction` types in Python, which interface to the LLVM [http://llvm.org/docs/ProgrammersManual.html#Module Module] and [http://llvm.org/docs/ProgrammersManual.html#Function Function] types. Teach `llvmmodule` to [http://llvm.org/doxygen/ReaderWriter_8h.html read bitcode files] and [http://llvm.org/doxygen/PrintModulePass_8h.html pretty-print them]. (Reading the bitcode files isn't really necessary for the next step, but it'll help with testing this one.)

[http://code.google.com/p/llvm-py/ llvm-py] has such wrapper types, but using Python-defined types in the core interpreter is very difficult, so we'll write our own. These won't be full wrappers (leave that to llvm-py). Just what's convenient and useful for debugging.</li>

<li>Add a `compile_llvm` builtin that uses most of the same code as the `compile` builtin but generates LLVM IR (as the `llvmmodule` or `llvmfunction` type) instead. We can get this working incrementally and keep a working Python interpreter the whole time. Along the way, we'll probably take each opcode and translate its ceval implementation into an IRBuilder implementation, but for difficult opcodes we could just emit calls to an interpretation function. We may want to/have to mix this step with the next one.
Several Python constructs may give us trouble here:
<ul>
<li>Generators: I don't have a plan for generators yet. We'd have to save the PC, registers, and locals, except that LLVM doesn't expose a PC, just labels, so we'd probably give each yield an index and switch on the current index to get back to it. And we could require that all registers have been spilled at that point, maybe. But I don't know the details of doing most of that. I'm hoping to figure it out when I get there.</li>
<li>Closures: A closure is just a function with a pre-bound parameter representing the environment.</li>
<li>ExceptionHandling</li>
</ul>
</li>

<li>Add a way to call `llvmfunction`s. The plain LLVM interpreter may not be able to call external functions by the time we get here. JITting may be slightly more difficult. We'll do whatever's easier. All we want is to run stuff. Write lots of tests.</li>

<li>Replace the Python-bytecode compiler with the new LLVM compiler, possibly under a flag. Talin suggests having both run and select the one to execute based on a flag in the code/function object. The flag will let us run regrtest over all the tests in both modes.</li>

<li>Figure out the simplest possible version of [PersistentIr .pyc persistence]. I'm hoping the obvious thing -- dump the python module as an LLVM Module in bitcode to disk -- just works. We can add the fancy profiling data and pre-optimized code later (step 12).</li>

<li>Teach Python how to decide whether to JIT or interpret a particular function. I imagine that interpretation will be faster for things that run once, and JITting will be faster for hot functions, and we'll have to tune a bit to find the right balance. We'll, of course, have to get both the JIT and interpreter working at this point. At this point I expect the interpreter to get faster than our original release.</li>

<li>Start adding [LlvmOptimizations optimizations].

Talin points out that it'll be hard to apply most of the pre-defined optimizations at first because Python calls are indirect. This motivates type inference so we can make the calls direct.</li>
</ol>

8+ can be tried in parallel, probably starting toward the end of 2009Q2.

<ol start="8">
<li>[DirectLlvmIrGeneration Generate better LLVM IR] than a direct translation from CPython bytecode would imply.</li>

<li>Figure out how to specialize functions for particular types, which allows better inlining. We should be able to do as well as psyco pretty easily since we can steal their algorithms.
<ol type="a">
<li>Allow users to annotate functions to tell the interpreter which specializations to use.</li>
<li>Profile calls and [TypeInference infer common argument types].</li>
</ol></li>

<li>If the optimizations become expensive enough that the JIT hurts our startup time, we can profile for expensive functions and [GradualOptimization only optimize them]. This resembles how HotSpot works.</li>

<li>9b and 10 require us to be able to keep multiple versions of code around for any given function and backpatch existing callers. We'll probably want to [GarbageCollectingCode GC the multiple versions], which should be interesting.</li>

<li>[PersistentIr Persist the generated and optimized IR to disk] in place of .pyc
files, perhaps along with any gathered profile data. This allows subsequent runs to take advantage of the optimization work previous runs have already done. Matches LLVM's promise to be "A Compilation Framework for Lifelong Program Analysis & Transformation". :)</li>
</ol>